{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21329139",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Knet, Test, Base.Iterators, Printf, LinearAlgebra, Random, CUDA, IterTools, DelimitedFiles, Statistics\n",
    "Knet.atype() = KnetArray{Float32}\n",
    "#Knet.atype() = Array{Float32}\n",
    "using Images, TestImages, OffsetArrays, Colors\n",
    "using Plots\n",
    "using JLD2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8048bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"models/transformer.jl\")\n",
    "include(\"dataloader/dataloader.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a9d6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Knet.array_type[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe79c62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read  files\n",
    "annotations = readdlm(\"archive/annotations_train.txt\", '\\t', String, '\\n')[2:end,2:end]\n",
    "\n",
    "# prepare vocab object\n",
    "bow = \"\"\n",
    "for sent in annotations[:,2]#\n",
    "    bow *= sent\n",
    "end\n",
    "#bow = split(bow)\n",
    "# bow wil be one giant sentence, corpus.\n",
    "v = Vocab(bow)\n",
    "\n",
    "# Initialize Iterators\n",
    "img_iterator = ImgReader(\"archive/img_dirs_train.txt\",load_and_process)\n",
    "tgt_iterator = TextReader(\"archive/ann_caps_train.txt\",v)\n",
    "\n",
    "# simulate retrieving one batch\n",
    "img_state, tgt_state = nothing, nothing\n",
    "flag = true\n",
    "iter = 1\n",
    "batch = get_next_batch(img_iterator, tgt_iterator, img_state, tgt_state)\n",
    "\n",
    "batch_imgs, batch_indices, img_state, tgt_state = batch\n",
    "batch_imgs, batch_indices, labels = prepare_batch(batch, tgt_iterator)\n",
    "println(iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abc1f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build Image Caption Generating Transformer\n",
    "transformer = Transformer(512, length(tgt_iterator.vocab.i2w), tgt_iterator.vocab.eos, false)\n",
    "\n",
    "# simulate forward training pass without autograd- returns loss value\n",
    "transformer(batch_imgs, batch_indices, labels)\n",
    "\n",
    "# simulate forward training pass with autograd- returns Tape object T\n",
    "loss = @diff transformer(batch_imgs, batch_indices, labels; p=0.1, use_smooth_loss=true)\n",
    "\n",
    "# updates\n",
    "for p in params(transformer)\n",
    "    #p .-= 0.1 .* grad(loss, p)\n",
    "    diff_p = grad(loss, p)\n",
    "    if diff_p == nothing\n",
    "        continue\n",
    "    else\n",
    "        p .= p - (0.1 .* diff_p)\n",
    "    end\n",
    "end\n",
    "\n",
    "# simulate inference\n",
    "#sample_img, pred_sent = transformer(batch_imgs[:,:,:,1:1]);\n",
    "word_probs = transformer(batch_imgs, batch_indices; p=0.1);\n",
    "# println(tgt_iterator.vocab.i2w[pred_sent]);\n",
    "# colorview(RGB, Array(sample_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ecef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax(smooth_labels[:,4,3] .== positive_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1dcde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapslices(argmax,Array(word_probs),dims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0c5955",
   "metadata": {},
   "outputs": [],
   "source": [
    "nll(word_probs, labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fdf473",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels2 = labels[:,:]\n",
    "labels2[labels2 .== 0] .= 1\n",
    "labels2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e9eaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OVERFIT TO SINGLE BATCH\n",
    "# normalize images\n",
    "batch_imgs_nomalized = 2.0 .* batch_imgs .- 1.0\n",
    "\n",
    "transformer_conv = Transformer(128, length(tgt_iterator.vocab.i2w), tgt_iterator.vocab.eos, true)\n",
    "transformer_mlp = Transformer(512, length(tgt_iterator.vocab.i2w), tgt_iterator.vocab.eos, false)\n",
    "losses_conv = []\n",
    "losses_mlp = []\n",
    "for iter in 1:1:8000\n",
    "    loss_conv = @diff transformer_conv(batch_imgs_nomalized, batch_indices, labels; p=0.2)\n",
    "    loss_mlp = @diff transformer_mlp(batch_imgs_nomalized, batch_indices, labels; p=0.2)\n",
    "\n",
    "    # updates\n",
    "    for p in params(transformer_conv)\n",
    "        diff_p = grad(loss_conv, p)\n",
    "        if diff_p == nothing\n",
    "            continue\n",
    "        else\n",
    "            p .= p - (0.01 .* diff_p)\n",
    "        end\n",
    "    end\n",
    "    for p in params(transformer_mlp)\n",
    "        diff_p = grad(loss_mlp, p)\n",
    "        if diff_p == nothing\n",
    "            continue\n",
    "        else\n",
    "            p .= p - (0.001 .* diff_p)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    println(\"iter: \", iter, \"/8000\")#, \"loss: \", value(loss_conv))\n",
    "    push!(losses_conv, value(loss_conv))\n",
    "    push!(losses_mlp, value(loss_mlp))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4247ff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OVERFIT TO SINGLE BATCH USING ADAM OPTIMIZER\n",
    "# transformer_conv = Transformer(128, length(tgt_iterator.vocab.i2w), tgt_iterator.vocab.eos, true)\n",
    "# for p in params(transformer_conv)\n",
    "#     p.opt = Adam(lr=compute_lr(128, 1; warmup_steps=4000), beta1=0.9, beta2=0.98, eps=1e-9, gclip=0)\n",
    "# end\n",
    "\n",
    "transformer_mlp = Transformer(512, length(tgt_iterator.vocab.i2w), tgt_iterator.vocab.eos, false)\n",
    "init_optimizer(transformer_mlp)\n",
    "# for p in params(transformer_mlp)\n",
    "#     p.opt = Adam(lr=compute_lr(512, 1; warmup_steps=4000), beta1=0.9, beta2=0.98, eps=1e-9, gclip=0)\n",
    "# end\n",
    "\n",
    "losses_conv = []\n",
    "losses_mlp = []\n",
    "\n",
    "# normalize images\n",
    "batch_imgs_nomalized = 2.0 .* batch_imgs .- 1.0\n",
    "\n",
    "for iter in 1:1:8000\n",
    "    #loss_conv = @diff transformer_conv(batch_imgs_nomalized, batch_indices, labels; p=0.2, use_smooth_loss=false)\n",
    "    loss_mlp = @diff transformer_mlp(batch_imgs_nomalized, batch_indices, labels; p=0.1, use_smooth_loss=true)\n",
    "\n",
    "    # updates\n",
    "#     ler=compute_lr(128, iter; warmup_steps=4000)\n",
    "#     for p in params(transformer_conv)\n",
    "#         g = grad(loss_conv, p)\n",
    "#         if g == nothing\n",
    "#             continue\n",
    "#         else\n",
    "#             update!(p, g)\n",
    "#             #p .= p - (ler .* g)\n",
    "#         end\n",
    "#     end\n",
    "\n",
    "#     ler=compute_lr(512, iter; warmup_steps=4000)\n",
    "#     for p in params(transformer_mlp)\n",
    "#         g = grad(loss_mlp, p)\n",
    "#         if g == nothing\n",
    "#             continue\n",
    "#         else\n",
    "#             #p.opt.lr=ler # update the lr attribute of the parameter's Adam optimizer object\n",
    "#             #update!(p, g)\n",
    "#             p .= p - (ler .* g)\n",
    "#         end\n",
    "#     end\n",
    "    \n",
    "    warm_adam_update(loss_mlp, transformer_mlp, iter)\n",
    "\n",
    "    println(\"iter: \", iter, \"/8000\")#, \"loss: \", value(loss_conv))\n",
    "#     push!(losses_conv, value(loss_conv))\n",
    "    push!(losses_mlp, value(loss_mlp))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc440df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([losses_mlp], labels=[\"mlp\"],xlabel=\"iterations\",ylabel=\"NLL Loss\")\n",
    "# plot([losses_conv, losses_mlp], labels=[\"conv\" \"mlp\"],xlabel=\"iterations\",ylabel=\"NLL Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec39705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate inference\n",
    "sample_img, pred_sent = transformer_conv(batch_imgs[:,:,:,1:1]);\n",
    "println(tgt_iterator.vocab.i2w[pred_sent]);\n",
    "colorview(RGB, Array(sample_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31959e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate inference\n",
    "sample_img, pred_sent = transformer_mlp(batch_imgs[:,:,:,1:1]);\n",
    "println(tgt_iterator.vocab.i2w[pred_sent]);\n",
    "colorview(RGB, Array(sample_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4d43fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSPECTION\n",
    "# we have batch_imgs 400x400x3*20\n",
    "# we have batch_indices 5*20\n",
    "# we have labels 5*20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd52288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the words in batch form (NOT autoregressively)\n",
    "word_probs = transformer_mlp(batch_imgs, batch_indices);\n",
    "\n",
    "println(\"batch indices input:\");\n",
    "show(stdout, \"text/plain\", batch_indices);\n",
    "println(\"\\n\");\n",
    "\n",
    "println(\"word probs argmax ie transformer output:\");\n",
    "show(stdout, \"text/plain\", Array(mapslices(argmax,Array(word_probs),dims=1)[1,:,:]));\n",
    "println(\"\\n\");\n",
    "\n",
    "println(\"labels:\")\n",
    "show(stdout, \"text/plain\", labels);\n",
    "println(\"\\n\");\n",
    "\n",
    "labels2 = labels[:,:];\n",
    "labels2[labels2.==0] .= 1;\n",
    "println(\"labels2:\")\n",
    "show(stdout, \"text/plain\", labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea404b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "si = 18 # a number [1,20] to select which sample from the batch we want to investigate\n",
    "\n",
    "# show gt caption\n",
    "println(\"GT caption\");\n",
    "println(tgt_iterator.vocab.i2w[labels2[:,si]]);\n",
    "println(\"\\n\");\n",
    "\n",
    "# convert predicted word prob argmax to english\n",
    "println(\"predicted (non-autoregressive) caption\");\n",
    "println(tgt_iterator.vocab.i2w[mapslices(argmax,Array(word_probs),dims=1)[1,:,si]]);\n",
    "println(\"\\n\");\n",
    "\n",
    "# show image\n",
    "colorview(RGB, Array(permutedims(batch_imgs[:,:,:,si],(3,1,2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628e86e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN TRAINING SCRIPT  - 13 minutes/epoch\n",
    "# img_iterator = ImgReader(\"archive/img_dirs.txt\",load_and_process)\n",
    "# tgt_iterator = TextReader(\"archive/ann_caps.txt\",v)\n",
    "\n",
    "# Dataset has 8000 images\n",
    "# Each batch is 4 images (4 unqiue, each with 5 captions, so batch size is actually 20)\n",
    "# So there will be 1:2000 iterations for one epoch\n",
    "\n",
    "exp_dir = \"resfreeze\"\n",
    "mkpath(\"$exp_dir/ckpts\")\n",
    "mkpath(\"$exp_dir/losses\")\n",
    "\n",
    "#transformer_conv = Transformer(128, length(tgt_iterator.vocab.i2w), tgt_iterator.vocab.eos, true)\n",
    "#transformer_mlp_128 = Transformer(128, length(tgt_iterator.vocab.i2w), tgt_iterator.vocab.eos, false)\n",
    "transformer_conv_512 = Transformer(512, length(tgt_iterator.vocab.i2w), tgt_iterator.vocab.eos, true)\n",
    "#init_optimizer(transformer_conv)\n",
    "#init_optimizer(transformer_mlp_128)\n",
    "init_optimizer(transformer_conv_512)\n",
    "\n",
    "#losses_conv = []\n",
    "#losses_mlp_128 = []\n",
    "losses_conv_512 = [] # stores the losses across entire training session, one average per iteration/minibatch\n",
    "val_losses = []     # stores the average loss across entire val dataset, one per epoch\n",
    "train_losses = []   # stores the average loss across entire train dataset, one per epoch\n",
    "iter = 1\n",
    "for epoch in 1:1:50\n",
    "    # ONE TRAINING EPOCH LOOP\n",
    "    train_loss = []\n",
    "    println(\"\\n\",\" epoch: \", epoch, \"\\n\")\n",
    "    \n",
    "    img_state, tgt_state = nothing, nothing\n",
    "    img_iterator = ImgReader(\"archive/img_dirs_train.txt\",load_and_process)\n",
    "    tgt_iterator = TextReader(\"archive/ann_caps_train.txt\",v)\n",
    "    while true #iter < 100 #flag !== nothing\n",
    "        batch = get_next_batch(img_iterator, tgt_iterator, img_state, tgt_state)\n",
    "        if batch == nothing\n",
    "            break\n",
    "        end\n",
    "        batch_imgs, batch_indices, img_state, tgt_state = batch\n",
    "        batch_imgs, batch_indices, labels = prepare_batch(batch, tgt_iterator)\n",
    "        batch_imgs_nomalized = 2.0 .* batch_imgs .- 1.0\n",
    "\n",
    "        #loss_conv = @diff transformer_conv(batch_imgs, batch_indices, labels, true)\n",
    "        #loss_mlp_128 = @diff transformer_mlp_128(batch_imgs, batch_indices, labels, true)\n",
    "        loss_conv_512 = @diff transformer_conv_512(batch_imgs_nomalized, batch_indices, labels; p=0.1, use_smooth_loss=true)\n",
    "\n",
    "        # updates\n",
    "        #warm_adam_update(loss_conv, transformer_conv, iter)\n",
    "        #warm_adam_update(loss_mlp_128, transformer_mlp_128, iter)\n",
    "        warm_adam_update(loss_conv_512, transformer_conv_512, iter; freeze=true)\n",
    "\n",
    "        println(\"iter: \", iter%2000, \"/2000 \", \"loss: \", value(loss_conv_512))\n",
    "        #push!(losses_conv, value(loss_conv))\n",
    "        #push!(losses_mlp_128, value(loss_mlp_128))\n",
    "        push!(losses_conv_512, value(loss_conv_512))\n",
    "        push!(train_loss, value(loss_conv_512))\n",
    "        iter = iter + 1\n",
    "    end\n",
    "    push!(train_losses, mean(train_loss)) # to plot training loss, but per epoch, to compare against val\n",
    "    Knet.save(\"$exp_dir/ckpts/transformer_conv_512_epoch$epoch.jld2\",\"transformer\",transformer_conv_512)\n",
    "    \n",
    "    # ONE VALIDATION EPOCH LOOP\n",
    "    val_loss = []\n",
    "    img_state, tgt_state = nothing, nothing\n",
    "    img_iterator = ImgReader(\"archive/img_dirs_val.txt\",load_and_process)\n",
    "    tgt_iterator = TextReader(\"archive/ann_caps_val.txt\",v)\n",
    "    while true #iter < 100 #flag !== nothing\n",
    "        batch = get_next_batch(img_iterator, tgt_iterator, img_state, tgt_state)\n",
    "        if batch == nothing\n",
    "            break\n",
    "        end\n",
    "        batch_imgs, batch_indices, img_state, tgt_state = batch\n",
    "        batch_imgs, batch_indices, labels = prepare_batch(batch, tgt_iterator)\n",
    "        batch_imgs_nomalized = 2.0 .* batch_imgs .- 1.0\n",
    "\n",
    "        #loss_conv = @diff transformer_conv(batch_imgs, batch_indices, labels, true)\n",
    "        #loss_mlp_128 = @diff transformer_mlp_128(batch_imgs, batch_indices, labels, true)\n",
    "        push!(val_loss, transformer_conv_512(batch_imgs_nomalized, batch_indices, labels; p=0.0, use_smooth_loss=true))\n",
    "    \n",
    "    end\n",
    "    push!(val_losses, mean(val_loss))\n",
    "end\n",
    "\n",
    "save_object(\"$exp_dir/losses/losses_conv_512.jld2\", losses_conv_512)\n",
    "save_object(\"$exp_dir/losses/val_losses.jld2\", val_losses)\n",
    "save_object(\"$exp_dir/losses/train_losses.jld2\", train_losses)\n",
    "\n",
    "# plot training loss - next cell\n",
    "#plot([losses_conv, losses_mlp_128, losses_mlp_512], labels=[\"conv\" \"mlp-128\" \"mlp-512\"],xlabel=\"iterations\",ylabel=\"NLL Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c013bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = load_object(\"$exp_dir/losses/train_losses.jld2\")\n",
    "val_losses = load_object(\"$exp_dir/losses/val_losses.jld2\")\n",
    "losses_conv_512 = load_object(\"$exp_dir/losses/losses_conv_512.jld2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f3a893",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot training loss\n",
    "#plot([losses_conv, losses_mlp_128, losses_mlp_512], labels=[\"conv\" \"mlp-128\" \"mlp-512\"],ylim=(0.0, 30.0),xlabel=\"iterations\",ylabel=\"NLL Loss\")\n",
    "plot([losses_conv_512], labels=[\"mlp-512\"],ylim=(0.0, 10.0),xlabel=\"iterations\",ylabel=\"label-smoothed CE Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d57276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([train_losses, val_losses], labels=[\"train losses\", \"val losses\"],ylim=(0.0, 10.0),xlabel=\"epoch\", ylabel=\"average label-smoothed CE Loss for whole epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf9fbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_conv_512 = Transformer(512, length(tgt_iterator.vocab.i2w), tgt_iterator.vocab.eos, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66912894",
   "metadata": {},
   "outputs": [],
   "source": [
    "Knet.save(\"transformer_conv_512_epoch0.jld2\", \"transformer\", transformer_conv_512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f92e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_conv_512 = Knet.load(\"ckpts4/transformer_conv_512_epoch100.jld2\",\"transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fc88e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predict the words in batch form (NOT autoregressively)\n",
    "batch_imgs_nomalized = 2.0 .* batch_imgs .- 1.0\n",
    "word_probs = transformer_conv_512(batch_imgs_nomalized, batch_indices);\n",
    "\n",
    "println(\"batch indices input:\");\n",
    "show(stdout, \"text/plain\", batch_indices);\n",
    "println(\"\\n\");\n",
    "\n",
    "println(\"word probs argmax ie transformer output:\");\n",
    "show(stdout, \"text/plain\", Array(mapslices(argmax,Array(word_probs),dims=1)[1,:,:]));\n",
    "println(\"\\n\");\n",
    "\n",
    "println(\"labels:\")\n",
    "show(stdout, \"text/plain\", labels);\n",
    "println(\"\\n\");\n",
    "\n",
    "labels2 = labels[:,:];\n",
    "labels2[labels2.==0] .= 1;\n",
    "println(\"labels2:\")\n",
    "show(stdout, \"text/plain\", labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76804128",
   "metadata": {},
   "outputs": [],
   "source": [
    "si = 2 # a number [1,20] to select which sample from the batch we want to investigate\n",
    "\n",
    "for si in 1:20\n",
    "    println(\"GT caption\");\n",
    "    println(tgt_iterator.vocab.i2w[labels2[:,si]]);\n",
    "    println(\"\\n\");\n",
    "\n",
    "    # convert predicted word prob argmax to english\n",
    "    println(\"predicted (non-autoregressive) caption\");\n",
    "    println(tgt_iterator.vocab.i2w[mapslices(argmax,Array(word_probs),dims=1)[1,:,si]]);\n",
    "    println(\"\\n\");\n",
    "\n",
    "    # run auto-regressive inference\n",
    "    _, pred_sent = transformer_conv_512(batch_imgs[:,:,:,si:si]);\n",
    "    println(\"predicted autoregressive caption\");\n",
    "    println(tgt_iterator.vocab.i2w[pred_sent]);\n",
    "    println(\"\\n\");\n",
    "end\n",
    "\n",
    "# show image\n",
    "colorview(RGB, Array(permutedims(batch_imgs[:,:,:,si],(3,1,2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59e470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate inference\n",
    "img_state, tgt_state = nothing, nothing\n",
    "batch = get_next_batch(img_iterator, tgt_iterator, img_state, tgt_state)\n",
    "\n",
    "batch_imgs, batch_indices, img_state, tgt_state = batch\n",
    "batch_imgs, batch_indices, labels = prepare_batch(batch, tgt_iterator)\n",
    "\n",
    "sample_img, pred_sent = transformer_mlp_512(batch_imgs[:,:,:,5:5]);\n",
    "println(tgt_iterator.vocab.i2w[pred_sent]);\n",
    "colorview(RGB, Array(sample_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674c4e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02ab8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125c258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "Knet.save(\"transformer_mlp_128_epoch10.jld2\",\"transformer\",transformer_mlp_128)\n",
    "Knet.save(\"transformer_mlp_512_epoch10.jld2\",\"transformer\",transformer_mlp_512)\n",
    "Knet.save(\"transformer_conv_128_epoch10.jld2\",\"transformer\",transformer_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64d8e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=3\n",
    "print(\"hello_epoch$epoch.jld2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ab0099",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "pretrained = Knet.load(\"transformer_mlp_512_epoch10.jld2\",\"transformer\")\n",
    "\n",
    "# simulate inference\n",
    "sample_img, pred_sent = pretrained(batch_imgs[:,:,:,1:1]);\n",
    "println(tgt_iterator.vocab.i2w[pred_sent]);\n",
    "colorview(RGB, Array(sample_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cc7251",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### INFERENCE DEBUGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0433d92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_probs = pretrained(batch_imgs, batch_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e7a8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c818294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapslices(argmax,Array(word_probs),dims=1)[1,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a15a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353786de",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_probs = pretrained(batch_imgs[:,:,:,1:1], reshape([1350, 65, 32],(3,1)))#batch_indices[1:1,1:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cf308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape([1350],(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5636095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapslices(argmax,Array(word_probs),dims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b100ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
