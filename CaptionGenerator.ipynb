{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21329139",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Knet, Test, Base.Iterators, Printf, LinearAlgebra, Random, CUDA, IterTools, DelimitedFiles, Statistics\n",
    "Knet.atype() = KnetArray{Float32}\n",
    "using Images, TestImages, OffsetArrays, Colors\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8048bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"models/transformer.jl\")\n",
    "include(\"dataloader/dataloader.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe79c62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read  files\n",
    "annotations = readdlm(\"archive/annotations_train.txt\", '\\t', String, '\\n')[2:end,2:end]\n",
    "\n",
    "# prepare vocab object\n",
    "bow = \"\"\n",
    "for sent in annotations[:,2]#\n",
    "    bow *= sent\n",
    "end\n",
    "#bow = split(bow)\n",
    "# bow wil be one giant sentence, corpus.\n",
    "v = Vocab(bow)\n",
    "\n",
    "# Initialize Iterators\n",
    "img_iterator = ImgReader(\"archive/img_dirs_train.txt\",load_and_process)\n",
    "tgt_iterator = TextReader(\"archive/ann_caps_train.txt\",v)\n",
    "\n",
    "# simulate retrieving one batch\n",
    "img_state, tgt_state = nothing, nothing\n",
    "flag = true\n",
    "iter = 1\n",
    "batch = get_next_batch(img_iterator, tgt_iterator, img_state, tgt_state)\n",
    "\n",
    "batch_imgs, batch_indices, img_state, tgt_state = batch\n",
    "batch_imgs, batch_indices, labels = prepare_batch(batch, tgt_iterator)\n",
    "println(iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abc1f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build Image Caption Generating Transformer\n",
    "transformer = Transformer(128, length(tgt_iterator.vocab.i2w), tgt_iterator.vocab.eos, false)\n",
    "\n",
    "# simulate forward training pass without autograd- returns loss value\n",
    "transformer(batch_imgs, batch_indices, labels)\n",
    "\n",
    "# simulate forward training pass with autograd- returns Tape object T\n",
    "loss = @diff transformer(batch_imgs, batch_indices, labels; p=0.1, use_smooth_loss=true)\n",
    "\n",
    "# updates\n",
    "for p in params(transformer)\n",
    "    #p .-= 0.1 .* grad(loss, p)\n",
    "    diff_p = grad(loss, p)\n",
    "    if diff_p == nothing\n",
    "        continue\n",
    "    else\n",
    "        p .= p - (0.1 .* diff_p)\n",
    "    end\n",
    "end\n",
    "\n",
    "# simulate inference\n",
    "#sample_img, pred_sent = transformer(batch_imgs[:,:,:,1:1]);\n",
    "word_probs = transformer(batch_imgs, batch_indices; p=0.1);\n",
    "# println(tgt_iterator.vocab.i2w[pred_sent]);\n",
    "# colorview(RGB, Array(sample_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a053134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "value(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61450cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fc68eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e051d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax(smooth_labels[:,4,3] .== positive_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1dcde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapslices(argmax,Array(word_probs),dims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0c5955",
   "metadata": {},
   "outputs": [],
   "source": [
    "nll(word_probs, labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fdf473",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels2 = labels[:,:]\n",
    "labels2[labels2 .== 0] .= 1\n",
    "labels2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e9eaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OVERFIT TO SINGLE BATCH\n",
    "# normalize images\n",
    "batch_imgs_nomalized = 2.0 .* batch_imgs .- 1.0\n",
    "\n",
    "transformer_conv = Transformer(128, length(tgt_iterator.vocab.i2w), tgt_iterator.vocab.eos, true)\n",
    "transformer_mlp = Transformer(512, length(tgt_iterator.vocab.i2w), tgt_iterator.vocab.eos, false)\n",
    "losses_conv = []\n",
    "losses_mlp = []\n",
    "for iter in 1:1:8000\n",
    "    loss_conv = @diff transformer_conv(batch_imgs_nomalized, batch_indices, labels; p=0.2)\n",
    "    loss_mlp = @diff transformer_mlp(batch_imgs_nomalized, batch_indices, labels; p=0.2)\n",
    "\n",
    "    # updates\n",
    "    for p in params(transformer_conv)\n",
    "        diff_p = grad(loss_conv, p)\n",
    "        if diff_p == nothing\n",
    "            continue\n",
    "        else\n",
    "            p .= p - (0.01 .* diff_p)\n",
    "        end\n",
    "    end\n",
    "    for p in params(transformer_mlp)\n",
    "        diff_p = grad(loss_mlp, p)\n",
    "        if diff_p == nothing\n",
    "            continue\n",
    "        else\n",
    "            p .= p - (0.001 .* diff_p)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    println(\"iter: \", iter, \"/8000\")#, \"loss: \", value(loss_conv))\n",
    "    push!(losses_conv, value(loss_conv))\n",
    "    push!(losses_mlp, value(loss_mlp))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4247ff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OVERFIT TO SINGLE BATCH USING ADAM OPTIMIZER\n",
    "# transformer_conv = Transformer(128, length(tgt_iterator.vocab.i2w), tgt_iterator.vocab.eos, true)\n",
    "# for p in params(transformer_conv)\n",
    "#     p.opt = Adam(lr=compute_lr(128, 1; warmup_steps=4000), beta1=0.9, beta2=0.98, eps=1e-9, gclip=0)\n",
    "# end\n",
    "\n",
    "transformer_mlp = Transformer(512, length(tgt_iterator.vocab.i2w), tgt_iterator.vocab.eos, false)\n",
    "for p in params(transformer_mlp)\n",
    "    p.opt = Adam(lr=compute_lr(512, 1; warmup_steps=4000), beta1=0.9, beta2=0.98, eps=1e-9, gclip=0)\n",
    "end\n",
    "\n",
    "losses_conv = []\n",
    "losses_mlp = []\n",
    "\n",
    "# normalize images\n",
    "batch_imgs_nomalized = 2.0 .* batch_imgs .- 1.0\n",
    "\n",
    "for iter in 1:1:40000\n",
    "    #loss_conv = @diff transformer_conv(batch_imgs_nomalized, batch_indices, labels; p=0.2, use_smooth_loss=false)\n",
    "    loss_mlp = @diff transformer_mlp(batch_imgs_nomalized, batch_indices, labels; p=0.1, use_smooth_loss=true)\n",
    "\n",
    "    # updates\n",
    "#     ler=compute_lr(128, iter; warmup_steps=4000)\n",
    "#     for p in params(transformer_conv)\n",
    "#         g = grad(loss_conv, p)\n",
    "#         if g == nothing\n",
    "#             continue\n",
    "#         else\n",
    "#             update!(p, g)\n",
    "#             #p .= p - (ler .* g)\n",
    "#         end\n",
    "#     end\n",
    "    ler=compute_lr(512, iter; warmup_steps=4000)\n",
    "    for p in params(transformer_mlp)\n",
    "        g = grad(loss_mlp, p)\n",
    "        if g == nothing\n",
    "            continue\n",
    "        else\n",
    "            #update!(p, g)\n",
    "            p .= p - (ler .* g)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    println(\"iter: \", iter, \"/20000\")#, \"loss: \", value(loss_conv))\n",
    "#     push!(losses_conv, value(loss_conv))\n",
    "    push!(losses_mlp, value(loss_mlp))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc440df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([losses_mlp], labels=[\"mlp\"],xlabel=\"iterations\",ylabel=\"NLL Loss\")\n",
    "# plot([losses_conv, losses_mlp], labels=[\"conv\" \"mlp\"],xlabel=\"iterations\",ylabel=\"NLL Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec39705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate inference\n",
    "sample_img, pred_sent = transformer_conv(batch_imgs[:,:,:,1:1]);\n",
    "println(tgt_iterator.vocab.i2w[pred_sent]);\n",
    "colorview(RGB, Array(sample_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31959e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate inference\n",
    "sample_img, pred_sent = transformer_mlp(batch_imgs[:,:,:,1:1]);\n",
    "println(tgt_iterator.vocab.i2w[pred_sent]);\n",
    "colorview(RGB, Array(sample_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfec6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSPECTION\n",
    "# we have batch_imgs 400x400x3*20\n",
    "# we have batch_indices 5*20\n",
    "# we have labels 5*20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aba550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the words in batch form (NOT autoregressively)\n",
    "word_probs = transformer_mlp(batch_imgs, batch_indices);\n",
    "\n",
    "println(\"batch indices input:\");\n",
    "show(stdout, \"text/plain\", batch_indices);\n",
    "println(\"\\n\");\n",
    "\n",
    "println(\"word probs argmax ie transformer output:\");\n",
    "show(stdout, \"text/plain\", Array(mapslices(argmax,Array(word_probs),dims=1)[1,:,:]));\n",
    "println(\"\\n\");\n",
    "\n",
    "println(\"labels:\")\n",
    "show(stdout, \"text/plain\", labels);\n",
    "println(\"\\n\");\n",
    "\n",
    "labels2 = labels[:,:];\n",
    "labels2[labels2.==0] .= 1;\n",
    "println(\"labels2:\")\n",
    "show(stdout, \"text/plain\", labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea404b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "si = 1 # a number [1,20] to select which sample from the batch we want to investigate\n",
    "\n",
    "# show gt caption\n",
    "println(\"GT caption\");\n",
    "println(tgt_iterator.vocab.i2w[labels2[:,si]]);\n",
    "println(\"\\n\");\n",
    "\n",
    "# convert predicted word prob argmax to english\n",
    "println(\"predicted (non-autoregressive) caption\");\n",
    "println(tgt_iterator.vocab.i2w[mapslices(argmax,Array(word_probs),dims=1)[1,:,si]]);\n",
    "println(\"\\n\");\n",
    "\n",
    "# show image\n",
    "colorview(RGB, Array(permutedims(batch_imgs[:,:,:,si],(3,1,2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd992771",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOTE THAT I HAVE TURNED OFF MASKING IN DECODER SELF ATTENTION\n",
    "# try skipping the q k v mlps\n",
    "# try by-passing the attention computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628e86e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN TRAINING SCRIPT  - 13 minutes/epoch\n",
    "# img_iterator = ImgReader(\"archive/img_dirs.txt\",load_and_process)\n",
    "# tgt_iterator = TextReader(\"archive/ann_caps.txt\",v)\n",
    "\n",
    "# Dataset has 8000 images\n",
    "# Each batch is 4 images (4 unqiue, each with 5 captions, so batch size is actually 20)\n",
    "# So there will be 1:2000 iterations for one epoch\n",
    "transformer_conv = Transformer(128, length(tgt_iterator.vocab.i2w), tgt_iterator.vocab.eos, true)\n",
    "transformer_mlp_128 = Transformer(128, length(tgt_iterator.vocab.i2w), tgt_iterator.vocab.eos, false)\n",
    "transformer_mlp_512 = Transformer(512, length(tgt_iterator.vocab.i2w), tgt_iterator.vocab.eos, false)\n",
    "\n",
    "losses_conv = []\n",
    "losses_mlp_128 = []\n",
    "losses_mlp_512 = []\n",
    "for epoch in 1:1:10\n",
    "    println(\"epoch: \", epoch)\n",
    "    iter = 1\n",
    "    img_state, tgt_state = nothing, nothing\n",
    "    img_iterator = ImgReader(\"archive/img_dirs_train.txt\",load_and_process)\n",
    "    tgt_iterator = TextReader(\"archive/ann_caps_train.txt\",v)\n",
    "    while true #iter < 100 #flag !== nothing\n",
    "        batch = get_next_batch(img_iterator, tgt_iterator, img_state, tgt_state)\n",
    "        if batch == nothing\n",
    "            break\n",
    "        end\n",
    "        batch_imgs, batch_indices, img_state, tgt_state = batch\n",
    "        batch_imgs, batch_indices, labels = prepare_batch(batch, tgt_iterator)\n",
    "\n",
    "        loss_conv = @diff transformer_conv(batch_imgs, batch_indices, labels)\n",
    "        loss_mlp_128 = @diff transformer_mlp_128(batch_imgs, batch_indices, labels)\n",
    "        loss_mlp_512 = @diff transformer_mlp_512(batch_imgs, batch_indices, labels)\n",
    "\n",
    "        # updates\n",
    "        for p in params(transformer_conv)\n",
    "            diff_p = grad(loss_conv, p)\n",
    "            if diff_p == nothing\n",
    "                continue\n",
    "            else\n",
    "                p .= p - (0.001 .* diff_p)\n",
    "            end\n",
    "        end\n",
    "        # updates\n",
    "        for p in params(transformer_mlp_128)\n",
    "            diff_p = grad(loss_mlp_128, p)\n",
    "            if diff_p == nothing\n",
    "                continue\n",
    "            else\n",
    "                p .= p - (0.001 .* diff_p)\n",
    "            end\n",
    "        end\n",
    "        # updates\n",
    "        for p in params(transformer_mlp_512)\n",
    "            diff_p = grad(loss_mlp_512, p)\n",
    "            if diff_p == nothing\n",
    "                continue\n",
    "            else\n",
    "                p .= p - (0.001 .* diff_p)\n",
    "            end\n",
    "        end\n",
    "\n",
    "        println(\"iter: \", 4*iter, \"/8000\", \"loss: \", value(loss_conv))\n",
    "        push!(losses_conv, value(loss_conv))\n",
    "        push!(losses_mlp_128, value(loss_mlp_128))\n",
    "        push!(losses_mlp_512, value(loss_mlp_512))\n",
    "        iter = iter + 1\n",
    "    end\n",
    "end\n",
    "\n",
    "# plot training loss - next cell\n",
    "#plot([losses_conv, losses_mlp_128, losses_mlp_512], labels=[\"conv\" \"mlp-128\" \"mlp-512\"],xlabel=\"iterations\",ylabel=\"NLL Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f3a893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training loss\n",
    "plot([losses_conv, losses_mlp_128, losses_mlp_512], labels=[\"conv\" \"mlp-128\" \"mlp-512\"],ylim=(0.0, 30.0),xlabel=\"iterations\",ylabel=\"NLL Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59e470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate inference\n",
    "img_state, tgt_state = nothing, nothing\n",
    "batch = get_next_batch(img_iterator, tgt_iterator, img_state, tgt_state)\n",
    "\n",
    "batch_imgs, batch_indices, img_state, tgt_state = batch\n",
    "batch_imgs, batch_indices, labels = prepare_batch(batch, tgt_iterator)\n",
    "\n",
    "sample_img, pred_sent = transformer_conv(batch_imgs[:,:,:,5:5]);\n",
    "println(tgt_iterator.vocab.i2w[pred_sent]);\n",
    "colorview(RGB, Array(sample_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674c4e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02ab8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125c258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "Knet.save(\"transformer_mlp_128_epoch10.jld2\",\"transformer\",transformer_mlp_128)\n",
    "Knet.save(\"transformer_mlp_512_epoch10.jld2\",\"transformer\",transformer_mlp_512)\n",
    "Knet.save(\"transformer_conv_128_epoch10.jld2\",\"transformer\",transformer_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ab0099",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "pretrained = Knet.load(\"transformer_mlp_512_epoch10.jld2\",\"transformer\")\n",
    "\n",
    "# simulate inference\n",
    "sample_img, pred_sent = pretrained(batch_imgs[:,:,:,1:1]);\n",
    "println(tgt_iterator.vocab.i2w[pred_sent]);\n",
    "colorview(RGB, Array(sample_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cc7251",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### INFERENCE DEBUGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0433d92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_probs = pretrained(batch_imgs, batch_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e7a8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c818294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapslices(argmax,Array(word_probs),dims=1)[1,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a15a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353786de",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_probs = pretrained(batch_imgs[:,:,:,1:1], reshape([1350, 65, 32],(3,1)))#batch_indices[1:1,1:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cf308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape([1350],(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5636095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapslices(argmax,Array(word_probs),dims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b100ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
